{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2b29e0-150c-47b3-9f56-e0241255ca87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NOTEBOOK_NAME = \"ex6-trn-add-desc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd0b75a-2889-48af-b1df-a47cb38584bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OUTPUT_DIR = f\"/notebooks/kaggle_lecr/output/{NOTEBOOK_NAME}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea3d0df-f4e8-4202-a89b-f5926005aa42",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch==1.12.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (1.12.0+cu116)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/rapids/lib/python3.9/site-packages (from torch==1.12.0) (4.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.12.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (0.12.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.20.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (4.20.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/rapids/lib/python3.9/site-packages (from transformers==4.20.1) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from transformers==4.20.1) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from transformers==4.20.1) (1.23.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from transformers==4.20.1) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from transformers==4.20.1) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/rapids/lib/python3.9/site-packages (from transformers==4.20.1) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from transformers==4.20.1) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from transformers==4.20.1) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from transformers==4.20.1) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.20.1) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->transformers==4.20.1) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->transformers==4.20.1) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->transformers==4.20.1) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->transformers==4.20.1) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"pip3 install torch==1.12.0 --extra-index-url https://download.pytorch.org/whl/cu116\")\n",
    "os.system(\"pip install tokenizers==0.12.1\")\n",
    "os.system(\"pip install transformers==4.20.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09c789d-506f-4886-b7ee-c222198479fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 22 16:05:31 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 26%   35C    P0    44W / 180W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6819dc6-ff40-4f79-8685-02db3c6a8f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/envs/rapids/lib/python3.9/site-packages (0.21.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('pip install python-dotenv')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c95534da-f206-4f29-a023-0b49a499bdf9",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.2.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (1.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from scikit-learn==1.2.1) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from scikit-learn==1.2.1) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from scikit-learn==1.2.1) (1.6.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from scikit-learn==1.2.1) (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"pip install scikit-learn==1.2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af13c3b",
   "metadata": {
    "papermill": {
     "duration": 7.561173,
     "end_time": "2023-02-11T02:05:08.553027",
     "exception": false,
     "start_time": "2023-02-11T02:05:00.991854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================================\n",
    "# Libraries\n",
    "# =========================================================================================\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f108c3d",
   "metadata": {
    "papermill": {
     "duration": 0.01567,
     "end_time": "2023-02-11T02:05:08.574968",
     "exception": false,
     "start_time": "2023-02-11T02:05:08.559298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Configurations\n",
    "# =========================================================================================\n",
    "class CFG:\n",
    "    debug = False\n",
    "    upload_data = True\n",
    "    wandb = True\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "    model = \"xlm-roberta-base\"\n",
    "    gradient_checkpointing = False\n",
    "    num_cycles = 0.5\n",
    "    warmup_ratio = 0.1\n",
    "    encoder_lr = 1e-5\n",
    "    decoder_lr = 1e-4\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 1632#128#64#32\n",
    "    weight_decay = 0.01\n",
    "    max_grad_norm = 0.012\n",
    "    max_len = 512\n",
    "    n_folds = 5\n",
    "    seed = 42\n",
    "    epochs = 5\n",
    "    data_url = \"/notebooks/kaggle_lecr/data/learning-equality-curriculum-recommendations\"\n",
    "    train_set_url = \"/notebooks/kaggle_lecr/output/ex6-uns-add-desc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf556033-94aa-4878-9379-1cd79a7bc14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ====================================================\n",
    "# # wandb\n",
    "# # ====================================================\n",
    "# if CFG.wandb:\n",
    "#     os.system('pip install wandb')\n",
    "#     import wandb\n",
    "\n",
    "#     try:\n",
    "#         # for kaggle\n",
    "#         # from kaggle_secrets import UserSecretsClient\n",
    "#         # user_secrets = UserSecretsClient()\n",
    "#         # secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "        \n",
    "#         # for paperspace\n",
    "#         secret_value_0 = os.getenv('WANDB_API_KEY')\n",
    "#         wandb.login(key=secret_value_0)\n",
    "        \n",
    "#         anony = None\n",
    "#     except:\n",
    "#         anony = \"must\"\n",
    "#         print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "\n",
    "#     def class2dict(f):\n",
    "#         return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "#     run = wandb.init(project='LECR',\n",
    "#                      entity=\"sinchir0\",\n",
    "#                      name=NOTEBOOK_NAME,\n",
    "#                      config=class2dict(CFG),\n",
    "#                      group=\"trn\",\n",
    "#                      job_type=\"train\",\n",
    "#                      anonymous=anony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aea8f99c",
   "metadata": {
    "papermill": {
     "duration": 0.014544,
     "end_time": "2023-02-11T02:05:08.595307",
     "exception": false,
     "start_time": "2023-02-11T02:05:08.580763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Seed everything for deterministic results\n",
    "# =========================================================================================\n",
    "def seed_everything(cfg):\n",
    "    random.seed(cfg.seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    torch.cuda.manual_seed(cfg.seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fd14795",
   "metadata": {
    "papermill": {
     "duration": 0.015773,
     "end_time": "2023-02-11T02:05:08.616987",
     "exception": false,
     "start_time": "2023-02-11T02:05:08.601214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Data Loading\n",
    "# =========================================================================================\n",
    "def read_data(cfg):\n",
    "    train = pd.read_csv(f\"{cfg.train_set_url}/train.csv\")\n",
    "    \n",
    "    topics = pd.read_csv(cfg.data_url + \"/\" + \"topics.csv\")\n",
    "    content = pd.read_csv(cfg.data_url + \"/\" + \"content.csv\")\n",
    "    correlations = pd.read_csv(cfg.data_url + \"/\" + \"correlations.csv\")\n",
    "\n",
    "    topics[\"title\"] = topics[\"title\"].fillna(\"\")\n",
    "    content[\"title\"] = content[\"title\"].fillna(\"\")\n",
    "    \n",
    "    topics[\"description\"] = topics[\"description\"].fillna(\"\")\n",
    "    content[\"description\"] = content[\"description\"].fillna(\"\")\n",
    "    \n",
    "    content['text'] = content['text'].fillna(\"\")\n",
    "    \n",
    "    print(' ')\n",
    "    print('-' * 50)\n",
    "    print(f\"train.shape: {train.shape}\")\n",
    "    print(f\"correlations.shape: {correlations.shape}\")\n",
    "    return train, topics, content, correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99c94a6b",
   "metadata": {
    "papermill": {
     "duration": 0.013152,
     "end_time": "2023-02-11T02:05:08.635842",
     "exception": false,
     "start_time": "2023-02-11T02:05:08.622690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(train: pd.DataFrame):\n",
    "    # Create feature column\n",
    "    # train['text'] = train['topics_titles'] + '[SEP]' + train['content_titles']\n",
    "    train['text'] = train['topics_texts'] + '[SEP]' + train['content_texts']\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d457e372",
   "metadata": {
    "papermill": {
     "duration": 0.017013,
     "end_time": "2023-02-11T02:05:08.658444",
     "exception": false,
     "start_time": "2023-02-11T02:05:08.641431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_cat_info(train: pd.DataFrame, topics: pd.DataFrame, content:pd.DataFrame):\n",
    "    merge_train = pd.merge(train, topics[[\"id\", \"level\", \"category\"]], left_on=\"topics_ids\", right_on=\"id\", how=\"left\")\n",
    "    merge_train = merge_train.drop(\"id\", axis=1)\n",
    "    merge_train = pd.merge(merge_train, content[[\"id\", \"kind\"]], left_on=\"content_ids\", right_on=\"id\", how=\"left\")\n",
    "    merge_train = merge_train.drop(\"id\", axis=1)\n",
    "    \n",
    "    merge_train[\"level_tag\"] = merge_train[\"level\"].apply(lambda x: f\"[LEVEL{x}]\")\n",
    "    merge_train[\"category_tag\"] = merge_train[\"category\"].apply(lambda x: f\"[CATEGORY_{x.upper()}]\")\n",
    "    merge_train[\"kind_tag\"] = merge_train[\"kind\"].apply(lambda x: f\"[KIND_{x.upper()}]\")\n",
    "    \n",
    "    level_tag_list = sorted(merge_train[\"level_tag\"].unique()) \n",
    "    category_list = sorted(merge_train[\"category_tag\"].unique()) \n",
    "    kind_list = sorted(merge_train[\"kind_tag\"].unique()) \n",
    "    \n",
    "    # train['topics_titles'] = merge_train['level_tag'] + merge_train[\"category_tag\"] + merge_train['topics_titles'] \n",
    "    train['topics_texts'] = merge_train['level_tag'] + merge_train[\"category_tag\"] + merge_train['topics_texts'] \n",
    "    # train['content_titles'] = merge_train['kind_tag'] + merge_train['content_titles']\n",
    "    train['content_texts'] = merge_train['kind_tag'] + merge_train['content_texts']\n",
    "    \n",
    "    return train, level_tag_list, category_list, kind_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a68033ec",
   "metadata": {
    "papermill": {
     "duration": 0.01453,
     "end_time": "2023-02-11T02:05:08.678562",
     "exception": false,
     "start_time": "2023-02-11T02:05:08.664032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# CV split\n",
    "# =========================================================================================\n",
    "def cv_split(train, cfg):\n",
    "    kfold = StratifiedGroupKFold(n_splits = cfg.n_folds, shuffle = True, random_state = cfg.seed)\n",
    "    for num, (train_index, val_index) in enumerate(kfold.split(train, train['target'], train['topics_ids'])):\n",
    "        train.loc[val_index, 'fold'] = int(num)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7007386",
   "metadata": {
    "papermill": {
     "duration": 0.016546,
     "end_time": "2023-02-11T02:05:08.702048",
     "exception": false,
     "start_time": "2023-02-11T02:05:08.685502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# F2 score metric\n",
    "# =========================================================================================\n",
    "def f2_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
    "    return round(f2.mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c79f24de",
   "metadata": {
    "papermill": {
     "duration": 0.015781,
     "end_time": "2023-02-11T02:05:08.723530",
     "exception": false,
     "start_time": "2023-02-11T02:05:08.707749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Get max length\n",
    "# =========================================================================================\n",
    "def get_max_length(train, cfg):\n",
    "    lengths = []\n",
    "    for text in tqdm(train['text'].fillna(\"\").values, total = len(train)):\n",
    "        length = len(cfg.tokenizer(text, add_special_tokens = False)['input_ids'])\n",
    "        lengths.append(length)\n",
    "    cfg.max_len = min(max(lengths) + 5, cfg.max_len) # cls + sep + level + category + kind\n",
    "    print(f\"max_len: {cfg.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc8d29cf",
   "metadata": {
    "papermill": {
     "duration": 0.058232,
     "end_time": "2023-02-11T02:05:08.787596",
     "exception": false,
     "start_time": "2023-02-11T02:05:08.729364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Prepare input, tokenize\n",
    "# =========================================================================================\n",
    "def prepare_input(text, cfg):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors = None, \n",
    "        add_special_tokens = True, \n",
    "        max_length = cfg.max_len,\n",
    "        pad_to_max_length = True,\n",
    "        truncation = True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs\n",
    "\n",
    "# =========================================================================================\n",
    "# Custom dataset\n",
    "# =========================================================================================\n",
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df['target'].values\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.texts[item], self.cfg)\n",
    "        label = torch.tensor(self.labels[item], dtype = torch.float)\n",
    "        return inputs, label\n",
    "    \n",
    "# =========================================================================================\n",
    "# Collate function for training\n",
    "# =========================================================================================\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs\n",
    "\n",
    "# =========================================================================================\n",
    "# Mean pooling class\n",
    "# =========================================================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "# =========================================================================================\n",
    "# Model\n",
    "# =========================================================================================\n",
    "class custom_model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states = True)\n",
    "        self.config.hidden_dropout = 0.0\n",
    "        self.config.hidden_dropout_prob = 0.0\n",
    "        self.config.attention_dropout = 0.0\n",
    "        self.config.attention_probs_dropout_prob = 0.0\n",
    "        self.model = AutoModel.from_pretrained(cfg.model, config = self.config)\n",
    "        # AutoTokenizer.from_pretrainedでadditional_special_tokensをした際は、増えたtoken分、新しい語彙として登録が必要らしい\n",
    "        # https://cocoinit23.com/pytorch-runtimeerror-cuda-error-device-side-assert-triggered/\n",
    "        self.model.resize_token_embeddings(len(cfg.tokenizer))\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output\n",
    "    \n",
    "# =========================================================================================\n",
    "# Helper functions\n",
    "# =========================================================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "# =========================================================================================\n",
    "# Train function loop\n",
    "# =========================================================================================\n",
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, target) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled = True):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, \n",
    "                          step, \n",
    "                          len(train_loader), \n",
    "                          remain = timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss = losses,\n",
    "                          grad_norm = grad_norm,\n",
    "                          lr = scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "# =========================================================================================\n",
    "# Valid function loop\n",
    "# =========================================================================================\n",
    "def valid_fn(valid_loader, model, criterion, device, cfg):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, target) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, \n",
    "                          len(valid_loader),\n",
    "                          loss = losses,\n",
    "                          remain = timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds, axis = 0)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "# =========================================================================================\n",
    "# Get best threshold\n",
    "# =========================================================================================\n",
    "def get_best_threshold(x_val, val_predictions, correlations):\n",
    "    best_score = 0\n",
    "    best_threshold = None\n",
    "    for thres in np.arange(0.001, 0.1, 0.001):\n",
    "        x_val['predictions'] = np.where(val_predictions > thres, 1, 0)\n",
    "        x_val1 = x_val[x_val['predictions'] == 1]\n",
    "        x_val1 = x_val1.groupby(['topics_ids'])['content_ids'].unique().reset_index()\n",
    "        x_val1['content_ids'] = x_val1['content_ids'].apply(lambda x: ' '.join(x))\n",
    "        x_val1.columns = ['topic_id', 'predictions']\n",
    "        x_val0 = pd.Series(x_val['topics_ids'].unique())\n",
    "        x_val0 = x_val0[~x_val0.isin(x_val1['topic_id'])]\n",
    "        x_val0 = pd.DataFrame({'topic_id': x_val0.values, 'predictions': \"\"})\n",
    "        x_val_r = pd.concat([x_val1, x_val0], axis = 0, ignore_index = True)\n",
    "        x_val_r = x_val_r.merge(correlations, how = 'left', on = 'topic_id')\n",
    "        score = f2_score(x_val_r['content_ids'], x_val_r['predictions'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = thres\n",
    "    return best_score, best_threshold\n",
    "    \n",
    "# =========================================================================================\n",
    "# Train & Evaluate\n",
    "# =========================================================================================\n",
    "def train_and_evaluate_one_fold(train, correlations, fold, cfg, add_topic_content: list):\n",
    "    print(' ')\n",
    "    print(f\"========== fold: {fold} training ==========\")\n",
    "    # Split train & validation\n",
    "    x_train = train[train['fold'] != fold]\n",
    "    x_val = train[train['fold'] == fold]\n",
    "    \n",
    "    # categoryがsourceのtopicは評価に使わない\n",
    "    x_val = x_val[~x_val[\"topics_texts\"].str.contains(\"CATEGORY_SOURCE\")]\n",
    "    \n",
    "    # 追加したpositiveのtopic, contentは評価には使わない\n",
    "    x_val = x_val[~(x_val[\"topics_ids\"] + x_val[\"content_ids\"]).isin(add_topic_content)]\n",
    "    \n",
    "    valid_labels = x_val['target'].values\n",
    "    train_dataset = custom_dataset(x_train, cfg)\n",
    "    valid_dataset = custom_dataset(x_val, cfg)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = True, \n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = False, \n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    # Get model\n",
    "    model = custom_model(cfg)\n",
    "    model.to(device)\n",
    "    # Optimizer\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay = 0.0):\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "            'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    optimizer_parameters = get_optimizer_params(\n",
    "        model, \n",
    "        encoder_lr = cfg.encoder_lr, \n",
    "        decoder_lr = cfg.decoder_lr,\n",
    "        weight_decay = cfg.weight_decay\n",
    "    )\n",
    "    optimizer = AdamW(\n",
    "        optimizer_parameters, \n",
    "        lr = cfg.encoder_lr, \n",
    "        eps = cfg.eps, \n",
    "        betas = cfg.betas\n",
    "    )\n",
    "    num_train_steps = int(len(x_train) / cfg.batch_size * cfg.epochs)\n",
    "    num_warmup_steps = num_train_steps * cfg.warmup_ratio\n",
    "    # Scheduler\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps = num_warmup_steps, \n",
    "        num_training_steps = num_train_steps, \n",
    "        num_cycles = cfg.num_cycles\n",
    "        )\n",
    "    # Training & Validation loop\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
    "    best_score = 0\n",
    "    for epoch in range(cfg.epochs):\n",
    "        start_time = time.time()\n",
    "        # Train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg)\n",
    "        # Validation\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device, cfg)\n",
    "        # Compute f2_score\n",
    "        score, threshold = get_best_threshold(x_val, predictions, correlations)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        print(f'Epoch {epoch+1} - Score: {score:.4f} - Threshold: {threshold:.5f}')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save(\n",
    "                {'model': model.state_dict(), 'predictions': predictions}, \n",
    "                f\"{OUTPUT_DIR}/{cfg.model.replace('/', '-')}_fold{fold}_{cfg.seed}.pth\"\n",
    "                )\n",
    "            val_predictions = predictions\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    # Get best threshold\n",
    "    best_score, best_threshold = get_best_threshold(x_val, val_predictions, correlations)\n",
    "    # Save Score, Threshold\n",
    "    score = {\"best_score\": best_score, \"best_threshold\": best_threshold}\n",
    "    with open(f\"{OUTPUT_DIR}/score.pkl\", \"wb\") as f:\n",
    "        pickle.dump(score, f)\n",
    "    print(f'Our CV score is {best_score} using a threshold of {best_threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3f57c04",
   "metadata": {
    "papermill": {
     "duration": 0.019865,
     "end_time": "2023-02-11T02:05:08.813594",
     "exception": false,
     "start_time": "2023-02-11T02:05:08.793729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "seed_everything(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75b78f49",
   "metadata": {
    "papermill": {
     "duration": 22.091749,
     "end_time": "2023-02-11T02:05:30.911554",
     "exception": false,
     "start_time": "2023-02-11T02:05:08.819805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "train.shape: (615170, 5)\n",
      "correlations.shape: (61517, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "train, topics, content, correlations = read_data(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0e6c17a-1e04-43e3-8522-645a7e50d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive sampleを足す\n",
    "def add_positive_sample(train: pd.DataFrame, correlations: pd.DataFrame, topics: pd.DataFrame, content: pd.DataFrame):\n",
    "    \n",
    "    topic_text_dict = dict(zip(topics[\"id\"], topics['title'] + \" \" + topics['description']))\n",
    "    content_text_dict = dict(zip(content[\"id\"], content['title'] + \" \" + content['description']))\n",
    "    \n",
    "    correlations[\"content_ids_list\"] = correlations[\"content_ids\"].apply(lambda x : x.split())\n",
    "    \n",
    "    all_positive_sample = correlations.explode(\"content_ids_list\")[[\"topic_id\",\"content_ids_list\"]]\n",
    "    all_positive_sample = all_positive_sample.rename(columns={\"topic_id\":\"topics_ids\",\"content_ids_list\":\"content_ids\"})\n",
    "    \n",
    "    all_positive_sample[\"topics_texts\"] = all_positive_sample[\"topics_ids\"].map(topic_text_dict)\n",
    "    all_positive_sample[\"content_texts\"] = all_positive_sample[\"content_ids\"].map(content_text_dict)\n",
    "    all_positive_sample[\"target\"] = 1\n",
    "    \n",
    "    all_positive_sample = all_positive_sample.reset_index(drop=True)\n",
    "    \n",
    "    # 追加するtopic, contentのみを持つlistを生成\n",
    "    all_positive_topic_content = (all_positive_sample[\"topics_ids\"] + all_positive_sample[\"content_ids\"]).tolist()\n",
    "    train_positive = train[train[\"target\"] == 1]\n",
    "    train_positive_topic_content = (train_positive[\"topics_ids\"] + train_positive[\"content_ids\"]).tolist()\n",
    "    add_topic_content = list(set(all_positive_topic_content) - set(train_positive_topic_content))\n",
    "    \n",
    "    # trainにpositive sampleを追加\n",
    "    train = pd.concat([train, all_positive_sample]).drop_duplicates(subset=[\"topics_ids\",\"content_ids\"], keep='first')\n",
    "    train = train.sort_values(\"topics_ids\")\n",
    "    train = train.reset_index(drop=True)\n",
    "\n",
    "    return train, add_topic_content\n",
    "\n",
    "train, add_topic_content = add_positive_sample(train, correlations, topics, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67fa349b-7b57-43d6-844e-533dbbeced8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics_ids</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>topics_texts</th>\n",
       "      <th>content_texts</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_431a13312468</td>\n",
       "      <td>Откриването на резисторите &gt; Открития и проект...</td>\n",
       "      <td>Успоредни резистори (част 2) Множество успоред...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_0b4a3ea959ba</td>\n",
       "      <td>Откриването на резисторите &gt; Открития и проект...</td>\n",
       "      <td>Последователни и успоредни резистори Идентифиц...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_8e6555906046</td>\n",
       "      <td>Откриването на резисторите &gt; Открития и проект...</td>\n",
       "      <td>Пример: Анализиране на по-сложна верига от рез...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_5bc0e1e2cba0</td>\n",
       "      <td>Откриването на резисторите &gt; Открития и проект...</td>\n",
       "      <td>Променлив резистор (реостат) с графит от молив...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_9847be21acc7</td>\n",
       "      <td>Откриването на резисторите &gt; Открития и проект...</td>\n",
       "      <td>Успоредни резистори (част 1) Резисторите са ус...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics_ids     content_ids  \\\n",
       "0  t_00004da3a1b2  c_431a13312468   \n",
       "1  t_00004da3a1b2  c_0b4a3ea959ba   \n",
       "2  t_00004da3a1b2  c_8e6555906046   \n",
       "3  t_00004da3a1b2  c_5bc0e1e2cba0   \n",
       "4  t_00004da3a1b2  c_9847be21acc7   \n",
       "\n",
       "                                        topics_texts  \\\n",
       "0  Откриването на резисторите > Открития и проект...   \n",
       "1  Откриването на резисторите > Открития и проект...   \n",
       "2  Откриването на резисторите > Открития и проект...   \n",
       "3  Откриването на резисторите > Открития и проект...   \n",
       "4  Откриването на резисторите > Открития и проект...   \n",
       "\n",
       "                                       content_texts  target  \n",
       "0  Успоредни резистори (част 2) Множество успоред...       0  \n",
       "1  Последователни и успоредни резистори Идентифиц...       0  \n",
       "2  Пример: Анализиране на по-сложна верига от рез...       0  \n",
       "3  Променлив резистор (реостат) с графит от молив...       1  \n",
       "4  Успоредни резистори (част 1) Резисторите са ус...       0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cacb8ef-7b40-44a8-9923-e811d089ff1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117651"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(add_topic_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27837264",
   "metadata": {
    "papermill": {
     "duration": 0.02252,
     "end_time": "2023-02-11T02:05:30.947474",
     "exception": false,
     "start_time": "2023-02-11T02:05:30.924954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    train = train[:1000]\n",
    "    CFG.epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4205a7c0",
   "metadata": {
    "papermill": {
     "duration": 2.095627,
     "end_time": "2023-02-11T02:05:33.051735",
     "exception": false,
     "start_time": "2023-02-11T02:05:30.956108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, level_tag_list, category_list, kind_list = merge_cat_info(train, topics, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03d9bbf0",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017352,
     "end_time": "2023-02-11T02:05:33.075436",
     "exception": false,
     "start_time": "2023-02-11T02:05:33.058084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[LEVEL0]',\n",
       " '[LEVEL10]',\n",
       " '[LEVEL1]',\n",
       " '[LEVEL2]',\n",
       " '[LEVEL3]',\n",
       " '[LEVEL4]',\n",
       " '[LEVEL5]',\n",
       " '[LEVEL6]',\n",
       " '[LEVEL7]',\n",
       " '[LEVEL8]',\n",
       " '[LEVEL9]']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0875c7d4",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015936,
     "end_time": "2023-02-11T02:05:33.097182",
     "exception": false,
     "start_time": "2023-02-11T02:05:33.081246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CATEGORY_ALIGNED]', '[CATEGORY_SOURCE]', '[CATEGORY_SUPPLEMENTAL]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28e80456",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015618,
     "end_time": "2023-02-11T02:05:33.119122",
     "exception": false,
     "start_time": "2023-02-11T02:05:33.103504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[KIND_AUDIO]',\n",
       " '[KIND_DOCUMENT]',\n",
       " '[KIND_EXERCISE]',\n",
       " '[KIND_HTML5]',\n",
       " '[KIND_VIDEO]']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kind_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb705e19",
   "metadata": {
    "papermill": {
     "duration": 0.227458,
     "end_time": "2023-02-11T02:05:33.352932",
     "exception": false,
     "start_time": "2023-02-11T02:05:33.125474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = preprocess(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c95af3cf",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.024865,
     "end_time": "2023-02-11T02:05:33.384259",
     "exception": false,
     "start_time": "2023-02-11T02:05:33.359394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [LEVEL4][CATEGORY_SOURCE]Откриването на резист...\n",
       "1         [LEVEL4][CATEGORY_SOURCE]Откриването на резист...\n",
       "2         [LEVEL4][CATEGORY_SOURCE]Откриването на резист...\n",
       "3         [LEVEL4][CATEGORY_SOURCE]Откриването на резист...\n",
       "4         [LEVEL4][CATEGORY_SOURCE]Откриването на резист...\n",
       "                                ...                        \n",
       "732816    [LEVEL2][CATEGORY_SOURCE]تحديد العلاقة بين الإ...\n",
       "732817    [LEVEL2][CATEGORY_SOURCE]تحديد العلاقة بين الإ...\n",
       "732818    [LEVEL2][CATEGORY_SOURCE]تحديد العلاقة بين الإ...\n",
       "732819    [LEVEL2][CATEGORY_SOURCE]تحديد العلاقة بين الإ...\n",
       "732820    [LEVEL2][CATEGORY_SOURCE]تحديد العلاقة بين الإ...\n",
       "Name: topics_texts, Length: 732821, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"topics_texts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09133fac",
   "metadata": {
    "papermill": {
     "duration": 8.251238,
     "end_time": "2023-02-11T02:05:41.664455",
     "exception": false,
     "start_time": "2023-02-11T02:05:33.413217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG.tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CFG.model,\n",
    "    additional_special_tokens = level_tag_list + category_list + kind_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc3017c7",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018461,
     "end_time": "2023-02-11T02:05:41.691787",
     "exception": false,
     "start_time": "2023-02-11T02:05:41.673326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[LEVEL0]',\n",
       " '[LEVEL10]',\n",
       " '[LEVEL1]',\n",
       " '[LEVEL2]',\n",
       " '[LEVEL3]',\n",
       " '[LEVEL4]',\n",
       " '[LEVEL5]',\n",
       " '[LEVEL6]',\n",
       " '[LEVEL7]',\n",
       " '[LEVEL8]',\n",
       " '[LEVEL9]',\n",
       " '[CATEGORY_ALIGNED]',\n",
       " '[CATEGORY_SOURCE]',\n",
       " '[CATEGORY_SUPPLEMENTAL]',\n",
       " '[KIND_AUDIO]',\n",
       " '[KIND_DOCUMENT]',\n",
       " '[KIND_EXERCISE]',\n",
       " '[KIND_HTML5]',\n",
       " '[KIND_VIDEO]']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.tokenizer.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b490a510-9e68-4203-b3a1-9bd5ce3c787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categoryがsourceのnegativeデータを減らし、計算時間を短くする\n",
    "train[\"is_category_source\"] = train[\"topics_texts\"].str.contains(\"CATEGORY_SOURCE\")\n",
    "train = train[~(train[\"is_category_source\"] & (train[\"target\"] == 0))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4871bf1d-8673-4cec-8472-7daa58e3e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算を終わらせるため、行を減らす\n",
    "if not CFG.debug:\n",
    "    train = train.sample(50000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e5a3ce5",
   "metadata": {
    "papermill": {
     "duration": 24.616901,
     "end_time": "2023-02-11T02:06:06.316153",
     "exception": false,
     "start_time": "2023-02-11T02:05:41.699252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CV split\n",
    "train = cv_split(train, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "287e8991-6d5a-470d-9459-2cc95718db13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    10195\n",
      "1    10037\n",
      "2    10031\n",
      "4     9909\n",
      "3     9828\n",
      "Name: fold, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[\"fold\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "494577d4-2513-465c-938c-c4d635829e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84d901da-9881-4008-8a1d-53655a9722ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    29489\n",
       "0    20511\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07e9b843",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 81.097871,
     "end_time": "2023-02-11T02:07:27.589260",
     "exception": false,
     "start_time": "2023-02-11T02:06:06.491389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76429d8cc1349469f10467400da34bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (671 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 512\n"
     ]
    }
   ],
   "source": [
    "# Get max length\n",
    "get_max_length(train, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "896807cf-cf68-42f9-aca1-118b255065e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    CFG.epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5253160",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 17391.060041,
     "end_time": "2023-02-11T06:57:18.656824",
     "exception": false,
     "start_time": "2023-02-11T02:07:27.596783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.20 GiB (GPU 0; 15.90 GiB total capacity; 14.22 GiB already allocated; 1.09 GiB free; 14.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_507/3815796829.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train and evaluate one fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_and_evaluate_one_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_topic_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_507/2310965426.py\u001b[0m in \u001b[0;36mtrain_and_evaluate_one_fold\u001b[0;34m(train, correlations, fold, cfg, add_topic_content)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mavg_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_507/2310965426.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_507/2310965426.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_507/2310965426.py\u001b[0m in \u001b[0;36mfeature\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mlast_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         )\n\u001b[0;32m--> 848\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 )\n\u001b[1;32m    523\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    525\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 336\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.20 GiB (GPU 0; 15.90 GiB total capacity; 14.22 GiB already allocated; 1.09 GiB free; 14.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Train and evaluate one fold\n",
    "train_and_evaluate_one_fold(train, correlations, 0, CFG, add_topic_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f858de-30f7-455a-856f-d9b1631f087f",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a8cd3-0dc7-453c-b37e-e94d125fc681",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"pip install kaggle\")\n",
    "os.system(\"mkdir -p ~/.kaggle/\")\n",
    "os.system(\"cp /notebooks/kaggle_lecr/kaggle.json ~/.kaggle/\")\n",
    "os.system(\"chmod 600 ~/.kaggle/kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a516b-6edc-4aac-88c5-93f58b6ece6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import json\n",
    "\n",
    "def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "    if \"_\" in dataset_name:\n",
    "        raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "    dataset_metadata = {}\n",
    "    dataset_metadata['id'] = f'sinchir0/{dataset_name}'\n",
    "    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n",
    "    dataset_metadata['title'] = dataset_name\n",
    "    with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')\n",
    "\n",
    "if CFG.upload_data:\n",
    "    print(f\"Create Dataset name:{NOTEBOOK_NAME}, output_dir:{OUTPUT_DIR}\")\n",
    "    dataset_create_new(dataset_name=NOTEBOOK_NAME, upload_dir=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465fda75-46ff-44dd-9bac-ec7fb5dc0e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879017b-061f-4d99-9f49-e59ab8b2884c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17548.763501,
   "end_time": "2023-02-11T06:57:21.580304",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-11T02:04:52.816803",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "218592928185498e9d59df2150307773": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a52632c7335f4547b3ac2e5873e80780",
        "IPY_MODEL_2ddfc44c7cdf4e50b90af044edb241e7",
        "IPY_MODEL_52fc9a4bd70c4d5d83cfda3bf9c96398"
       ],
       "layout": "IPY_MODEL_a83b337e4b59457a8ac5de9174f9cf9d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2ddfc44c7cdf4e50b90af044edb241e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8c155ce2411e4d16b964ab60e162b45c",
       "max": 615170,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9cc63961d65a4e05bc533f5a7c6301e6",
       "tabbable": null,
       "tooltip": null,
       "value": 615170
      }
     },
     "52fc9a4bd70c4d5d83cfda3bf9c96398": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_81d6d3f955d94a10a8a9563343a028a4",
       "placeholder": "​",
       "style": "IPY_MODEL_71acb0f9429c474a93233c1f859e53e2",
       "tabbable": null,
       "tooltip": null,
       "value": " 615170/615170 [01:21&lt;00:00, 3806.35it/s]"
      }
     },
     "71acb0f9429c474a93233c1f859e53e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "81d6d3f955d94a10a8a9563343a028a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c155ce2411e4d16b964ab60e162b45c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9cc63961d65a4e05bc533f5a7c6301e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a52632c7335f4547b3ac2e5873e80780": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f62b4326bfaf4b439f5b53f0eef9e80d",
       "placeholder": "​",
       "style": "IPY_MODEL_e26c5dc8f53343b399e78f5d540eaf0d",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "a83b337e4b59457a8ac5de9174f9cf9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e26c5dc8f53343b399e78f5d540eaf0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f62b4326bfaf4b439f5b53f0eef9e80d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
